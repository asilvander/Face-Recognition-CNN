{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Disclaimer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"This notebok created to test my Facial Emotions Recognition model on real images.\n\nI decided to test my model on politics faces and adjusted the categories labels for more fun.\n\nDisclaimer: All  labels on faces are made by machine that was trained on unbiased publically abailable dataset. Author may not agree with the machine conclusions :)","metadata":{}},{"cell_type":"markdown","source":"# Face detection","metadata":{}},{"cell_type":"code","source":"# face detection function using MTCNN pre-trained model\n\n!pip install mtcnn\n!apt-get update\n!apt-get install -y libgl1-mesa-glx\n\nimport cv2\nfrom mtcnn import MTCNN\nfrom IPython.display import Image\n\ndef detect_faces(image_path, detection_confidence=0.99, min_face_size=10):\n    # Load the image using OpenCV\n    image = cv2.imread(image_path)\n\n    # Create an MTCNN detector instance\n    detector = MTCNN()\n\n    # Use the detector to detect faces in the image\n    faces = detector.detect_faces(image)\n\n    # Draw a square around each face\n    for face in faces:\n        if face['confidence'] < detection_confidence:\n            continue\n        x, y, width, height = face['box']\n        if min(width, height) < min_face_size:\n            continue\n        face_size = max(width, height)\n        x, y = x + (width - face_size) // 2, y + (height - face_size) // 2\n        cv2.rectangle(image, (x, y), (x + face_size, y + face_size), (127, 255, 0), 1)\n\n    # Save the image with the detected faces to a file\n    cv2.imwrite(\"detected_faces.jpg\", image)\n\n    # Return the path to the saved file\n    return \"detected_faces.jpg\"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\n\n# Detect faces in the image of various emotions\nimage_path = detect_faces('/kaggle/input/testimages/pol7.jpg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:19:16.993580Z","iopub.execute_input":"2023-02-22T13:19:16.994232Z","iopub.status.idle":"2023-02-22T13:19:18.789987Z","shell.execute_reply.started":"2023-02-22T13:19:16.994183Z","shell.execute_reply":"2023-02-22T13:19:18.788843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Look, MTCNN model is unable to identify some faces","metadata":{}},{"cell_type":"markdown","source":"# Model for Emotions prediction","metadata":{}},{"cell_type":"code","source":"# let's see what is in our AffectNet dataset\n\nimport pandas as pd\nimport matplotlib.pyplot as plt  # plot\nimport os\nfrom os.path import join\n\npath = ('/kaggle/input/affectnet-training-data/')\nfile = (path + 'labels.csv')\ndf = pd.read_csv(file)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:07:19.002949Z","iopub.execute_input":"2023-02-22T13:07:19.004192Z","iopub.status.idle":"2023-02-22T13:07:19.098816Z","shell.execute_reply.started":"2023-02-22T13:07:19.004135Z","shell.execute_reply":"2023-02-22T13:07:19.097850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display random images\n\nimport random\n\nfig, axs = plt.subplots(2, 4, sharey=True, constrained_layout=True, num=None, \n                        figsize=(10, 5), dpi=80, facecolor='gray', edgecolor='k')\nfig.suptitle(\"Sample Faces and Labels\")\naxs = axs.flatten()\n\n\nfor i in range(8):\n    idx = random.randint(0, len(df)-1)  # randomly select an index\n    img_path = path + df['pth'][idx]\n    img = cv2.imread(img_path)  # read image\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # convert to BGR to RGB\n    axs[i].imshow(img)\n    axs[i].set_title(df['label'][idx])\n    axs[i].axis('off')","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:19:28.880663Z","iopub.execute_input":"2023-02-22T13:19:28.881003Z","iopub.status.idle":"2023-02-22T13:19:29.582116Z","shell.execute_reply.started":"2023-02-22T13:19:28.880972Z","shell.execute_reply":"2023-02-22T13:19:29.581198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"AffectNet is a large database of faces labeled by \"affects\" (psychological term for facial expressions). In order to accommodate common memory constraints, the resolution was reduced down to 96x96. Meaning that all images are exactly 96x96 pixels.","metadata":{}},{"cell_type":"markdown","source":"# Load images and label categories","metadata":{}},{"cell_type":"code","source":"# 1. define functions to pre-process and load images into arrays / label from folders\n\nimport cv2\nimport numpy as np\nfrom tensorflow.keras.utils import to_categorical\nimport os\n\nINPUT_PATH = \"/kaggle/input/affectnet-training-data/\"\nEMOTIONS = [f.name for f in os.scandir(INPUT_PATH) if f.is_dir()]\nIMAGE_SIZE = (96, 96)\n\nprint(EMOTIONS)\n\ndef image_generator(input_path, emotions, image_size):\n    for index, emotion in enumerate(emotions):\n        for filename in os.listdir(os.path.join(input_path, emotion)):\n            img = cv2.imread(os.path.join(input_path, emotion, filename))\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n            #img = cv2.resize(img, image_size)\n            #img = img.astype('float32') / 255.0  # Normilize\n            yield img, index\n\ndef load_images(input_path, emotions, image_size):\n    X, y = [], []\n    for img, label in image_generator(input_path, emotions, image_size):\n        X.append(img)\n        y.append(label)\n    X = np.array(X)\n    y = to_categorical(np.array(y))\n    return X, y","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:07:30.659373Z","iopub.execute_input":"2023-02-22T13:07:30.660586Z","iopub.status.idle":"2023-02-22T13:07:30.673391Z","shell.execute_reply.started":"2023-02-22T13:07:30.660496Z","shell.execute_reply":"2023-02-22T13:07:30.672586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the images \nX, y = load_images(INPUT_PATH, EMOTIONS, IMAGE_SIZE)\ninput_shape = X[0].shape\n#input_shape = (96,96,1) ","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:07:31.361383Z","iopub.execute_input":"2023-02-22T13:07:31.361957Z","iopub.status.idle":"2023-02-22T13:08:48.390573Z","shell.execute_reply.started":"2023-02-22T13:07:31.361923Z","shell.execute_reply":"2023-02-22T13:08:48.389508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's have some fun and adjust the emotions labels for our politics\n\nEMOTIONS = ['surprise', 'fear', 'neutral', 'sad', 'disgust', 'contempt', 'happy', 'anger']\n\nemotion_map = {\n    'surprise': 'Wow!',\n    'fear': 'Fear',\n    'neutral': \"Bored\",\n    'sad': 'No money',\n    'disgust': 'Disgust',\n    'contempt': 'Fake',\n    'happy': 'Money!',\n    'anger': 'Mad'\n}\n\nEMOTIONS = [emotion_map[e] for e in EMOTIONS]\nEMOTIONS","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:08:48.392695Z","iopub.execute_input":"2023-02-22T13:08:48.393009Z","iopub.status.idle":"2023-02-22T13:08:48.402354Z","shell.execute_reply.started":"2023-02-22T13:08:48.392980Z","shell.execute_reply":"2023-02-22T13:08:48.401109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# choose a random image index\nidx = np.random.randint(len(X))\n\n# display the image and its corresponding label from arrays\nplt.imshow(X[idx])\nplt.title(EMOTIONS[np.argmax(y[idx])])\nplt.axis('off')  # remove the grid\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:09:50.580137Z","iopub.execute_input":"2023-02-22T13:09:50.580792Z","iopub.status.idle":"2023-02-22T13:09:50.766050Z","shell.execute_reply.started":"2023-02-22T13:09:50.580732Z","shell.execute_reply":"2023-02-22T13:09:50.765156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train test split pre-processed data\n\n!pip install scikit-learn\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:09:33.017608Z","iopub.execute_input":"2023-02-22T13:09:33.018275Z","iopub.status.idle":"2023-02-22T13:09:42.033209Z","shell.execute_reply.started":"2023-02-22T13:09:33.017903Z","shell.execute_reply":"2023-02-22T13:09:42.032252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Emotions prediction model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"Num TPUs Available: \", len(tf.config.list_logical_devices('TPU')))\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:09:55.615612Z","iopub.execute_input":"2023-02-22T13:09:55.616811Z","iopub.status.idle":"2023-02-22T13:09:55.625428Z","shell.execute_reply.started":"2023-02-22T13:09:55.616745Z","shell.execute_reply":"2023-02-22T13:09:55.624250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# detect and init the TPU\n\nimport tensorflow as tf\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:10:24.065520Z","iopub.execute_input":"2023-02-22T13:10:24.065880Z","iopub.status.idle":"2023-02-22T13:10:28.625720Z","shell.execute_reply.started":"2023-02-22T13:10:24.065842Z","shell.execute_reply":"2023-02-22T13:10:28.624478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# our model_4 taken from Experiments notebook\n\n!pip install keras\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Conv2D, Dropout, BatchNormalization, Flatten, Dense, MaxPool2D\n    from tensorflow.keras.regularizers import l2\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    from tensorflow.keras.optimizers import Adam\n\n    model_4 = Sequential()\n\n    model_4.add(Conv2D(32, (3,3), activation=\"selu\", input_shape=input_shape))\n    model_4.add(BatchNormalization())\n    model_4.add(MaxPool2D(pool_size=(2,2)))\n    model_4.add(Dropout(0.3))\n\n    model_4.add(Conv2D(64, (3,3), activation=\"selu\"))\n    model_4.add(BatchNormalization())\n    model_4.add(Conv2D(64, (3,3), activation=\"selu\"))\n    model_4.add(BatchNormalization())\n    model_4.add(MaxPool2D(pool_size=(2,2)))\n    model_4.add(Dropout(0.4))\n\n    model_4.add(Conv2D(128, (3,3), activation=\"selu\"))\n    model_4.add(BatchNormalization())\n    model_4.add(Conv2D(128, (3,3), activation=\"selu\"))\n    model_4.add(BatchNormalization())\n    model_4.add(MaxPool2D(pool_size=(2,2)))\n    model_4.add(Dropout(0.5))\n\n    model_4.add(Conv2D(256, (3,3), activation=\"selu\"))\n    model_4.add(BatchNormalization())\n    model_4.add(Conv2D(256, (3,3), activation=\"selu\"))\n    model_4.add(BatchNormalization())\n    model_4.add(MaxPool2D(pool_size=(2,2)))\n    model_4.add(Dropout(0.6))\n\n    model_4.add(Flatten())\n    model_4.add(Dense(128, activation='selu', kernel_regularizer=l2(0.01)))\n    model_4.add(BatchNormalization())\n    model_4.add(Dropout(0.5))\n    model_4.add(Dense(8, activation='softmax'))\n\n    model_4.compile(optimizer=Adam(learning_rate=0.001), \n                    loss='categorical_crossentropy', \n                    metrics=['accuracy'],\n                    steps_per_execution=32)\n\n    model_4.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:10:31.648657Z","iopub.execute_input":"2023-02-22T13:10:31.648976Z","iopub.status.idle":"2023-02-22T13:10:41.635348Z","shell.execute_reply.started":"2023-02-22T13:10:31.648933Z","shell.execute_reply":"2023-02-22T13:10:41.634230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nBATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync\n\nhistory = model_4.fit(X_train, y_train, batch_size=BATCH_SIZE,\n                    epochs=30,\n                    validation_data=(X_test, y_test),\n                    \n                    callbacks = [EarlyStopping(patience=10, monitor='val_loss', mode='min'), \n                                 ReduceLROnPlateau(monitor='val_loss', \n                                                   factor=0.5, \n                                                   patience=2, \n                                                   verbose=1),\n                                 ModelCheckpoint('best_model.h5', \n                                                 save_best_only=True, \n                                                 save_weights_only=True, \n                                                 monitor='val_accuracy', \n                                                 mode='max')],\n                    verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:10:44.678033Z","iopub.execute_input":"2023-02-22T13:10:44.678388Z","iopub.status.idle":"2023-02-22T13:12:33.225192Z","shell.execute_reply.started":"2023-02-22T13:10:44.678354Z","shell.execute_reply":"2023-02-22T13:12:33.224003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot();","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:13:53.381248Z","iopub.execute_input":"2023-02-22T13:13:53.381608Z","iopub.status.idle":"2023-02-22T13:13:53.733416Z","shell.execute_reply.started":"2023-02-22T13:13:53.381577Z","shell.execute_reply":"2023-02-22T13:13:53.732321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculates the false positive rate, true positive rate, and AUC score\n\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nimport matplotlib.pyplot as plt\n\n# Make predictions\ny_pred = model_4.predict(X_test)\n\n# Compute ROC curve and ROC AUC for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(8):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC AUC score\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\nroc_auc[\"micro\"] = roc_auc_score(y_test, y_pred, multi_class='ovr')\n\n# Plot the ROC curves for each class and the micro-average ROC curve\nplt.figure(figsize=(8, 6))\nlw = 2\nplt.plot(fpr[\"micro\"], tpr[\"micro\"], lw=lw, label='micro-average ROC curve (AUC = {0:0.2f})'\n                                                   ''.format(roc_auc[\"micro\"]))\ncolors = ['cornflowerblue', 'darkorange', 'forestgreen', 'red', 'purple', 'gray', 'black', 'pink']\nfor i, color in zip(range(8), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of {0} (AUC = {1:0.2f})'.format(EMOTIONS[i], roc_auc[i]))\n    \nplt.plot([0, 1], [0, 1], color='gray', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=14)\nplt.ylabel('True Positive Rate', fontsize=14)\nplt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16)\nplt.legend(loc=\"lower right\", fontsize=12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:13:57.022465Z","iopub.execute_input":"2023-02-22T13:13:57.022808Z","iopub.status.idle":"2023-02-22T13:14:09.792630Z","shell.execute_reply.started":"2023-02-22T13:13:57.022776Z","shell.execute_reply":"2023-02-22T13:14:09.791484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute classification report\n\nfrom sklearn.metrics import classification_report\n\n# Convert one-hot encoded y_test back to integers\ny_test_int = np.argmax(y_test, axis=1)\n\n# Make predictions\ny_pred = model_4.predict(X_test)\n\n# Convert one-hot encoded y_pred back to integers\ny_pred_int = np.argmax(y_pred, axis=1)\n\n# Generate classification report\nprint(classification_report(y_test_int, y_pred_int))\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:14:19.910114Z","iopub.execute_input":"2023-02-22T13:14:19.911026Z","iopub.status.idle":"2023-02-22T13:14:22.090472Z","shell.execute_reply.started":"2023-02-22T13:14:19.910983Z","shell.execute_reply":"2023-02-22T13:14:22.089193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model\n\nmodel_4.save('/kaggle/working/model_4.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Detect + predict","metadata":{}},{"cell_type":"code","source":"# function that use MTCNN face detection and pass detected area to our model_4 for recognition\n\nimport cv2\nimport numpy as np\nfrom mtcnn import MTCNN\nfrom IPython.display import Image\n\ndef detect_faces_emo(image_path, detection_confidence=0.99, min_face_size=10):\n    # Load the image using OpenCV\n    image = cv2.imread(image_path)\n\n    # Create an MTCNN detector instance\n    detector = MTCNN()\n\n    # Use the detector to detect faces in the image\n    faces = detector.detect_faces(image)\n\n    # Loop over the detected faces\n    for face in faces:\n        # Check the confidence score of the detection\n        if face['confidence'] < detection_confidence:\n            continue\n        # Extract the bounding box coordinates\n        x, y, width, height = face['box']\n        # Check the size of the bounding box\n        if min(width, height) < min_face_size:\n            continue\n        \n        # Extract the face region from the image\n        face_image = image[y:y+height, x:x+width]\n        # Resize the face image to 96x96\n        face_image_resized = cv2.resize(face_image, (96, 96))\n        #face_image_gray = cv2.cvtColor(face_image_resized, cv2.COLOR_BGR2GRAY)\n        # Reshape the face image to match the input shape of the model\n        face_image_reshaped = face_image_resized.reshape((1, 96, 96, 3))\n        #if np.max(face_image_reshaped) > 1: face_image_reshaped = face_image_reshaped / 255\n        \n        # Use the model to predict the emotion of the face\n        predicted_emo = model_4.predict(face_image_reshaped)[0]\n        \n        # Draw the predicted emotion label on the rectangle around the face\n        label = EMOTIONS[np.argmax(predicted_emo)]\n        label_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n        label_x = x\n        label_y = y - 10\n        cv2.rectangle(image, (label_x - 5, label_y - label_size[1] - 5), (label_x + label_size[0] + 5, label_y + 5), (204, 229, 255), -1)\n        cv2.putText(image, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1, cv2.LINE_AA)\n\n\n\n    # Save the image with the detected faces and predicted emotions to a file\n    cv2.imwrite(\"detected_faces.jpg\", image)\n\n    # Return the path to the saved file\n    return \"detected_faces.jpg\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:14:42.188858Z","iopub.execute_input":"2023-02-22T13:14:42.189672Z","iopub.status.idle":"2023-02-22T13:14:42.202256Z","shell.execute_reply.started":"2023-02-22T13:14:42.189621Z","shell.execute_reply":"2023-02-22T13:14:42.201008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the new image of various politics\nimage_path = detect_faces_emo('/kaggle/input/testimages/pol7.jpg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:14:42.558767Z","iopub.execute_input":"2023-02-22T13:14:42.559144Z","iopub.status.idle":"2023-02-22T13:14:47.995950Z","shell.execute_reply.started":"2023-02-22T13:14:42.559105Z","shell.execute_reply":"2023-02-22T13:14:47.995212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adjusted emotions function","metadata":{}},{"cell_type":"code","source":"# now let's modify our function to draw the emotion probability out of all 8 emotions\n\nimport cv2\n\ndef detect_faces_emo_2(image_path, detection_confidence=0.99, min_face_size=10):\n    # Load the image using OpenCV\n    image = cv2.imread(image_path)\n\n    # Create an MTCNN detector instance\n    detector = MTCNN()\n\n    # Use the detector to detect faces in the image\n    faces = detector.detect_faces(image)\n\n    # Loop over the detected faces\n    for face in faces:\n        # Check the confidence score of the detection\n        if face['confidence'] < detection_confidence:\n            continue\n        # Extract the bounding box coordinates\n        x, y, width, height = face['box']\n        # Check the size of the bounding box\n        if min(width, height) < min_face_size:\n            continue\n        \n        # Extract the face region from the image\n        face_image = image[y:y+height, x:x+width]\n        # Resize the face image to 96x96\n        face_image_resized = cv2.resize(face_image, (96, 96))\n        # Reshape the face image to match the input shape of the model\n        face_image_reshaped = face_image_resized.reshape((1, 96, 96, 3))\n        # Use the model to predict the emotion of the face\n        predicted_emo = model_4.predict(face_image_reshaped)[0]\n        predicted_emo_sorted = sorted(list(enumerate(predicted_emo)), key=lambda x: x[1], reverse=True)\n        \n        # Extract the predicted probabilities for each emotion category\n        # Extract the predicted probabilities for each emotion category\n        probabilities = [\"{}\".format(round(prob * 100)) for index, prob in predicted_emo_sorted]\n        \n        # Draw the predicted emotion label on the rectangle around the face\n        label = EMOTIONS[np.argmax(predicted_emo)]\n        label_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n        label_x = x\n        label_y = y - 10\n        cv2.rectangle(image, (label_x - 5, label_y - label_size[1] - 5), (label_x + label_size[0] + 5, label_y + 5), (204, 229, 255), -1)\n        cv2.putText(image, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1, cv2.LINE_AA)\n        \n        # Draw a square rectangle around the face\n        face_size = min(width, height)\n        x_center = x + int(width / 2)\n        y_center = y + int(height / 2)\n        x1 = x_center - int(face_size / 2)\n        y1 = y_center - int(face_size / 2)\n        x2 = x_center + int(face_size / 2)\n        y2 = y_center + int(face_size / 2)\n        cv2.rectangle(image, (x1, y1), (x2, y2), (127, 255, 0), 2)\n\n        # Draw a vertical table with the predicted emotion probabilities\n        table_x, table_y = x1, y2 + 10\n        for index, prob in predicted_emo_sorted:\n            table_y += 10\n            emotion = EMOTIONS[index]\n            cv2.putText(image, emotion, (table_x, table_y), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n            cv2.putText(image, \"{}%\".format(round(prob * 100)), (table_x + 50, table_y), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n    \n    # Save the image with the detected faces and predicted emotions to a file\n    cv2.imwrite(\"detected_faces.jpg\", image)\n\n    # Return the path to the saved file\n    return \"detected_faces.jpg\"\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:15:19.991974Z","iopub.execute_input":"2023-02-22T13:15:19.992469Z","iopub.status.idle":"2023-02-22T13:15:20.010980Z","shell.execute_reply.started":"2023-02-22T13:15:19.992430Z","shell.execute_reply":"2023-02-22T13:15:20.009827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the image of politics\nimage_path = detect_faces_emo_2('/kaggle/input/testimages/pol7.jpg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:15:20.673358Z","iopub.execute_input":"2023-02-22T13:15:20.673655Z","iopub.status.idle":"2023-02-22T13:15:26.809605Z","shell.execute_reply.started":"2023-02-22T13:15:20.673625Z","shell.execute_reply":"2023-02-22T13:15:26.808569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the image of politics\nimage_path = detect_faces_emo_2('/kaggle/input/testimages/pol6.jpg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:16:12.567388Z","iopub.execute_input":"2023-02-22T13:16:12.568140Z","iopub.status.idle":"2023-02-22T13:16:15.634813Z","shell.execute_reply.started":"2023-02-22T13:16:12.568081Z","shell.execute_reply":"2023-02-22T13:16:15.633841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the image of politics\nimage_path = detect_faces_emo_2('/kaggle/input/testimages/pol10.jpg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:16:51.070100Z","iopub.execute_input":"2023-02-22T13:16:51.070581Z","iopub.status.idle":"2023-02-22T13:16:51.568101Z","shell.execute_reply.started":"2023-02-22T13:16:51.070549Z","shell.execute_reply":"2023-02-22T13:16:51.566404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the image of politics\nimage_path = detect_faces_emo_2('/kaggle/input/testimages/pol12.jpg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:17:12.955794Z","iopub.execute_input":"2023-02-22T13:17:12.957339Z","iopub.status.idle":"2023-02-22T13:17:14.119007Z","shell.execute_reply.started":"2023-02-22T13:17:12.957286Z","shell.execute_reply":"2023-02-22T13:17:14.117607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What if we train the model on politicians only? \n\nWhat if we process the video data and add time dimension to the model? (LSTM)\n\nWhat if we design the feature called \"Lie detection\" based on facial expressions in certain situations?\n\nWhat are your ideas? Please comment and vote!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}